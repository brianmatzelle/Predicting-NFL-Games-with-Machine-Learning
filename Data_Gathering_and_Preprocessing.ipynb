{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering and Pre-processing\n",
    "\n",
    "Import the required libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nfl_data_py as nfl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering\n",
    "\n",
    "NFL play by play data found [here](https://github.com/nflverse/nflverse-data/releases/tag/pbp). In this cell we will convert the csv files to pkl files, so they're easier to work with. This part may take a while, as the csv files are pretty big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seasons = []\n",
    "all_drives = []\n",
    "starting_row = None\n",
    "# loop through the play by play csv files\n",
    "for i in range(1999, 2023):\n",
    "    # open the csv file for the year (i is the year)\n",
    "    with open(f'./pbp_csv/play_by_play_{i}.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "\n",
    "        all_games = []\n",
    "        game = []\n",
    "        drive = []\n",
    "        drive_counter = 1\n",
    "        # starting_row first row of the csv\n",
    "        starting_row = next(reader)\n",
    "        # loop through the rows of the csv\n",
    "        for row in reader:\n",
    "            # skip the first row\n",
    "            if row[0] == 'play_id':\n",
    "                continue\n",
    "            # skip\n",
    "            elif row[27] == 'GAME':\n",
    "                continue\n",
    "            # add the row to the game's dataframe, until the 'desc' column contains END GAME\n",
    "            # desc is in column AB, which is index 27\n",
    "            elif row[27] == 'END GAME':\n",
    "                # add game to the list of dataframes and clear the game dataframe\n",
    "                game = pd.DataFrame(game)\n",
    "                game.columns = starting_row\n",
    "                all_games.append(game)\n",
    "                all_drives.append(drive)\n",
    "                game = []\n",
    "                drive_counter = 1\n",
    "            else:\n",
    "                if row[18] and drive_counter == int(row[18]):\n",
    "                    drive.append(row)\n",
    "                else:\n",
    "                    all_drives.append(drive)\n",
    "                    drive_counter += 1\n",
    "                    drive = []\n",
    "                    drive.append(row)\n",
    "                    \n",
    "                # add the row to the game list\n",
    "                game.append(row)\n",
    "        \n",
    "        all_seasons.append(all_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_be_dropped = [\n",
    "    'old_game_id', \n",
    "    'desc', \n",
    "    'play_id',\n",
    "    'game_date',\n",
    "    'ydsnet',\n",
    "    'no_score_prob',\n",
    "    'opp_fg_prob',\n",
    "    'opp_safety_prob',\n",
    "    'opp_td_prob',\n",
    "    'fg_prob',\n",
    "    'safety_prob',\n",
    "    'td_prob',\n",
    "    'extra_point_prob',\n",
    "    'two_point_conversion_prob',\n",
    "    'ep',\n",
    "    'epa',\n",
    "    'total_home_epa',\n",
    "    'total_away_epa',\n",
    "    'total_home_rush_epa',\n",
    "    'total_away_rush_epa',\n",
    "    'total_home_pass_epa',\n",
    "    'total_away_pass_epa',\n",
    "    'air_epa',\n",
    "    'yac_epa',\n",
    "    'comp_air_epa',\n",
    "    'comp_yac_epa',\n",
    "    'total_home_comp_air_epa',\n",
    "    'total_away_comp_air_epa',\n",
    "    'total_home_comp_yac_epa',\n",
    "    'total_away_comp_yac_epa',\n",
    "    'total_home_raw_air_epa',\n",
    "    'total_away_raw_air_epa',\n",
    "    'total_home_raw_yac_epa',\n",
    "    'total_away_raw_yac_epa',\n",
    "    'wp',\n",
    "    'def_wp',\n",
    "    'home_wp',\n",
    "    'away_wp',\n",
    "    'wpa',\n",
    "    'vegas_wpa',\n",
    "    'vegas_home_wpa',\n",
    "    'home_wp_post',\n",
    "    'away_wp_post',\n",
    "    'vegas_wp',\n",
    "    'vegas_home_wp',\n",
    "    'total_home_rush_wpa',\n",
    "    'total_away_rush_wpa',\n",
    "    'total_home_pass_wpa',\n",
    "    'total_away_pass_wpa',\n",
    "    'air_wpa',\n",
    "    'yac_wpa',\n",
    "    'comp_air_wpa',\n",
    "    'comp_yac_wpa',\n",
    "    'total_home_comp_air_wpa',\n",
    "    'total_away_comp_air_wpa',\n",
    "    'total_home_comp_yac_wpa',\n",
    "    'total_away_comp_yac_wpa',\n",
    "    'total_home_raw_air_wpa',\n",
    "    'total_away_raw_air_wpa',\n",
    "    'total_home_raw_yac_wpa',\n",
    "    'total_away_raw_yac_wpa',\n",
    "    'passer_player_id',\n",
    "    'passer_player_name',\n",
    "    'receiver_player_id',\n",
    "    'receiver_player_name',\n",
    "    'rusher_player_id',\n",
    "    'rusher_player_name',\n",
    "    'lateral_receiver_player_id',\n",
    "    'lateral_receiver_player_name',\n",
    "    'lateral_rusher_player_id',\n",
    "    'lateral_rusher_player_name',\n",
    "    'lateral_sack_player_id',\n",
    "    'lateral_sack_player_name',\n",
    "    'interception_player_id',\n",
    "    'interception_player_name',\n",
    "    'lateral_interception_player_id',\n",
    "    'lateral_interception_player_name',\n",
    "    'punt_returner_player_id',\n",
    "    'punt_returner_player_name',\n",
    "    'lateral_punt_returner_player_id',\n",
    "    'lateral_punt_returner_player_name',\n",
    "    'kickoff_returner_player_name',\n",
    "    'kickoff_returner_player_id',\n",
    "    'lateral_kickoff_returner_player_id',\n",
    "    'lateral_kickoff_returner_player_name',\n",
    "    'punter_player_id',\n",
    "    'punter_player_name',\n",
    "    'kicker_player_name',\n",
    "    'kicker_player_id',\n",
    "    'own_kickoff_recovery_player_id',\n",
    "    'own_kickoff_recovery_player_name',\n",
    "    'blocked_player_id',\n",
    "    'blocked_player_name',\n",
    "    'tackle_for_loss_1_player_id',\n",
    "    'tackle_for_loss_1_player_name',\n",
    "    'tackle_for_loss_2_player_id',\n",
    "    'tackle_for_loss_2_player_name',\n",
    "    'qb_hit_1_player_id',\n",
    "    'qb_hit_1_player_name',\n",
    "    'qb_hit_2_player_id',\n",
    "    'qb_hit_2_player_name',\n",
    "    'forced_fumble_player_1_player_id',\n",
    "    'forced_fumble_player_1_player_name',\n",
    "    'forced_fumble_player_2_player_id',\n",
    "    'forced_fumble_player_2_player_name',\n",
    "    'solo_tackle_1_player_id',\n",
    "    'solo_tackle_1_player_name',\n",
    "    'solo_tackle_2_player_id',\n",
    "    'solo_tackle_2_player_name',\n",
    "    'assist_tackle_1_player_id',\n",
    "    'assist_tackle_1_player_name',\n",
    "    'assist_tackle_2_player_id',\n",
    "    'assist_tackle_2_player_name',\n",
    "    'assist_tackle_3_player_id',\n",
    "    'assist_tackle_3_player_name',\n",
    "    'assist_tackle_4_player_id',\n",
    "    'assist_tackle_4_player_name',\n",
    "    'tackle_with_assist_1_player_id',\n",
    "    'tackle_with_assist_1_player_name',\n",
    "    'tackle_with_assist_2_player_id',\n",
    "    'tackle_with_assist_2_player_name',\n",
    "    'pass_defense_1_player_id',\n",
    "    'pass_defense_1_player_name',\n",
    "    'pass_defense_2_player_id',\n",
    "    'pass_defense_2_player_name',\n",
    "    'fumbled_1_player_id',\n",
    "    'fumbled_1_player_name',\n",
    "    'fumbled_2_player_id',\n",
    "    'fumbled_2_player_name',\n",
    "    'fumble_recovery_1_player_id',\n",
    "    'fumble_recovery_1_player_name',\n",
    "    'fumble_recovery_2_player_id',\n",
    "    'fumble_recovery_2_player_name',\n",
    "    'sack_player_id',\n",
    "    'sack_player_name',\n",
    "    'half_sack_1_player_id',\n",
    "    'half_sack_1_player_name',\n",
    "    'half_sack_2_player_id',\n",
    "    'half_sack_2_player_name',\n",
    "    'penalty_player_id',\n",
    "    'penalty_player_name',\n",
    "    'drive_quarter_end',\n",
    "    'fixed_drive',\n",
    "    'fixed_drive_result',\n",
    "    'drive_time_of_possession',\n",
    "    'drive_play_count',\n",
    "    'drive_first_downs',\n",
    "    'drive_inside20',\n",
    "    'drive_ended_with_score',\n",
    "    'drive_game_clock_end',\n",
    "    'drive_end_yard_line',\n",
    "    'drive_play_id_started',\n",
    "    'drive_play_id_ended',\n",
    "    'away_score',\n",
    "    'home_score',\n",
    "    'result',\n",
    "    'total',\n",
    "    'total_line',\n",
    "    'spread_line',\n",
    "    'passer',\n",
    "    'passer_jersey_number',\n",
    "    'rusher',\n",
    "    'rusher_jersey_number',\n",
    "    'receiver',\n",
    "    'receiver_jersey_number',\n",
    "    'passer_id',\n",
    "    'rusher_id',\n",
    "    'receiver_id',\n",
    "    'name',\n",
    "    'jersey_number',\n",
    "    'id',\n",
    "    'fantasy_player_id',\n",
    "    'fantasy_player_name',\n",
    "    'fantasy',\n",
    "    'fantasy_id',\n",
    "    'qb_epa',\n",
    "    'xyac_epa',\n",
    "    'pass_oe'\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make list of all target drives, and make all_drives into a list of dataframes\n",
    "\n",
    "### This cell takes 30+ minutes to execute on my M2 mac, since it's grabbing every drive since 1999 and writing the drives, and targets to PKL files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of all target drives\n",
    "all_target_drives = []\n",
    "all_drives_cleaned = []\n",
    "for drive in all_drives:\n",
    "    drive = pd.DataFrame(drive)\n",
    "    drive.columns = starting_row\n",
    "    print(drive['game_date'])\n",
    "    target_drive = pd.DataFrame()\n",
    "    target_drive['game_id'] = drive['game_id']\n",
    "    target_drive['fixed_drive'] = drive['fixed_drive']\n",
    "    target_drive['fixed_drive_result'] = drive['fixed_drive_result']\n",
    "    target_drive['drive_time_of_possession'] = drive['drive_time_of_possession']\n",
    "    target_drive['drive_play_count'] = drive['drive_play_count']\n",
    "    target_drive['drive_first_downs'] = drive['drive_first_downs']\n",
    "    target_drive['drive_inside20'] = drive['drive_inside20']\n",
    "    target_drive['drive_ended_with_score'] = drive['drive_ended_with_score']\n",
    "    target_drive['drive_game_clock_end'] = drive['drive_game_clock_end']\n",
    "    target_drive['drive_end_yard_line'] = drive['drive_end_yard_line']\n",
    "    all_target_drives.append(target_drive)\n",
    "    drive.drop(columns=columns_to_be_dropped, inplace=True)\n",
    "    all_drives_cleaned.append(drive)\n",
    "\n",
    "print('making all_drives_cleaned pkl')\n",
    "# write all_drives and all_target_drives to pickle files for later use\n",
    "with open('all_drives_cleaned.pickle', 'wb') as f:\n",
    "    pickle.dump(all_drives_cleaned, f)\n",
    "\n",
    "print('making all_target_drives pkl')\n",
    "with open('all_target_drives.pickle', 'wb') as f:\n",
    "    pickle.dump(all_target_drives, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all_drives_cleaned pkl file into variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all_drives_cleaned pkl to txt file to check\n",
    "with open('all_drives_cleaned.pickle', 'rb') as f:\n",
    "    all_drives_cleaned = pickle.load(f)\n",
    "print('all_drives_cleaned pkl loaded')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make 2007 season PKL file for simple ML\n",
    "2007 is my favorite season :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_drives_2007_season = []\n",
    "for drive in all_drives_cleaned:\n",
    "    # if the first four characters of drive['game_id'] are 2007, append to all_drives_2007_season\n",
    "    if drive['game_id'][0][0:4] == '2007':\n",
    "        all_drives_2007_season.append(drive)\n",
    "    elif int(drive['game_id'][0][0:4]) > 2007:\n",
    "        break\n",
    "print('all_drives_2007_season created')\n",
    "\n",
    "# dump all_drives_2007_season to pickle file\n",
    "with open('all_drives_2007_season.pickle', 'wb') as f:\n",
    "    pickle.dump(all_drives_2007_season, f)\n",
    "print('all_drives_2007_season pkl created')\n",
    "\n",
    "all_target_drives_2007_season = []\n",
    "# do the same for 2007 target drives\n",
    "for drive in all_target_drives:\n",
    "    if drive['game_id'][0][0:4] == '2007':\n",
    "        all_target_drives_2007_season.append(drive)\n",
    "    elif int(drive['game_id'][0][0:4]) > 2007:\n",
    "        break\n",
    "print('all_target_drives_2007_season created')\n",
    "\n",
    "# dump all_target_drives_2007_season to pickle file\n",
    "with open('all_target_drives_2007_season.pickle', 'wb') as f:\n",
    "    pickle.dump(all_target_drives_2007_season, f)\n",
    "print('all_target_drives_2007_season pkl created')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here if you already have the all_drives_2007_season.pickle file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize each column that needs to be normalized\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load preprocessed data\n",
    "with open('all_drives_cleaned.pickle', 'rb') as f:\n",
    "    all_drives_cleaned = pickle.load(f)\n",
    "\n",
    "with open('all_target_drives.pickle', 'rb') as f:\n",
    "    all_target_drives = pickle.load(f)\n",
    "\n",
    "# Let's assume 'fixed_drive_result' and 'drive_end_yard_line' are your target variables\n",
    "# Concatenate all drives into single dataframes\n",
    "df_drives = pd.concat(all_drives_cleaned)\n",
    "df_target = pd.concat(all_target_drives)[['fixed_drive_result', 'drive_end_yard_line']]\n",
    "\n",
    "# Split your data into training and test sets (80/20 split for example)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_drives, df_target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define your model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=[len(X_train.keys())]),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(2)  # Assuming two target variables\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='mean_squared_error',  # Suitable for continuous targets\n",
    "    optimizer=tf.keras.optimizers.RMSprop(0.001)\n",
    ")\n",
    "\n",
    "# Split your data into training and test sets (80/20 split for example)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_drives, df_target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10, \n",
    "    validation_split = 0.2\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "print(\"Testing set Loss: {:5.2f} \".format(loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfl",
   "language": "python",
   "name": "nfl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
